{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NASA CMR and GIBS\n",
    "\n",
    "This notebook looks into NASA CMR (Common Metadata Repository) data. \n",
    "**CMR** is 'one stop shopping' for metadata on NASA remote sensing resources (earth only).\n",
    "The CMR API is accessible via the python-cmr package; so we work through that a bit here.\n",
    "**GIBS** is the Global Image Browser System, one stop shopping for imagery; not covered here \n",
    "*mostly* (see below\\*). Also it is important to know that various other NASA projects have \n",
    "produced different portals. In particular Goddard Space Flight Center (GSFC) through \n",
    "GES DISC (Goddard Earth Science Data and Information Services Center)\n",
    "have produced a data web app called [Giovanni]( http://giovanni.gsfc.nasa.gov/giovanni/).\n",
    "\n",
    "What NASA ocean data coincides with the cabled array?  MODIS SST would be a good starting example. \n",
    "\n",
    "\n",
    "#### Links\n",
    "\n",
    "* [Jupyter notebook editing shortcuts](https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/)\n",
    "* [CMR](https://earthdata.nasa.gov/about/science-system-description/eosdis-components/common-metadata-repository)\n",
    "* [GIBS](https://pypi.python.org/pypi/python-cmr/0.3.1)\n",
    "* [Python-CMR](https://pypi.python.org/pypi/python-cmr/0.3.1)\n",
    "* [NEP NASA Earth Observations WMS resource page](https://neo.sci.gsfc.nasa.gov/about/wms.php)\n",
    "\n",
    "\n",
    "\n",
    "\\* but note: It is possible to search for 'NASA WMS' to find NASA (often GSFC) Web Mapping Services. These can be \n",
    "added as layers to an ipyleaflet Map. (Kilroy: Restore notes on GetCapabilities and layer qualifiers and whether\n",
    "a particular resource works reasonably well...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Python utility code\n",
    "from pathlib import Path\n",
    "home = str(Path.home()) + '/'\n",
    "data = home + 'data/'             # A non-repository location for datasets of interest\n",
    "\n",
    "def dirobj(obj): return [x for x in dir(obj) if not x.startswith('__')]\n",
    "\n",
    "def lsal(path=''):\n",
    "    import os\n",
    "    return os.popen('ls -al ' + path).readlines()\n",
    "\n",
    "def ShowGitHubImage(username, repo, folder, source, localcopyname, width, height):\n",
    "    import requests, shutil\n",
    "    from PIL import Image\n",
    "    outf = home + localcopyname\n",
    "    f = 'https://raw.githubusercontent.com/' + username + '/' + repo + '/master/' + folder + '/' + source\n",
    "    a = requests.get(f, stream = True)\n",
    "    if a.status_code == 200:\n",
    "        with open(outf, 'wb') as f:\n",
    "            a.raw.decode_content = True\n",
    "            shutil.copyfileobj(a.raw, f)\n",
    "    return Image.open(outf).resize((width,height),Image.ANTIALIAS)\n",
    "\n",
    "def ShowLocalImage(pathfromhome, filename, width, height):\n",
    "    import shutil\n",
    "    from PIL import Image\n",
    "    global home\n",
    "    f = home + '/' + pathfromhome + '/' + filename \n",
    "    return Image.open(f).resize((width,height),Image.ANTIALIAS)\n",
    "\n",
    "# Test either of the 'Show Image' functions\n",
    "# ShowGitHubImage('robfatland', 'othermathclub', 'images/cellular', 'conus_textile_shell_2.png', 'ctextile.jpg', 450, 250)\n",
    "# ShowLocalImage('.', 'ctextile.jpg', 450, 250)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da4cbabad3d47fd9b1d53ebf32c22e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=70, continuous_update=False, description='opacity'), Output()), _dom_cla…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.ShowMap>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The goal is to overlay an image (created earlier; .png file) with a controllable opacity\n",
    "#   ...but it doesn't work\n",
    "from ipyleaflet import Map, ImageOverlay\n",
    "from ipywidgets import *\n",
    "from traitlets import dlink\n",
    "\n",
    "thisMap = Map(center=(47, -129), zoom=5, layout=Layout(width='100%', height='600px'))\n",
    "\n",
    "def ShowMap(opacity):\n",
    "    mapOpacity = float(int(opacity))/100.\n",
    "    image = ImageOverlay(url=\"/home/jovyan/data/images/modis_sst.png\", bounds=((53.82, -138.58), (40.22, -122.45)), opacity=mapOpacity)\n",
    "    thisMap.add_layer(image)\n",
    "    thisMap\n",
    "    \n",
    "interact(ShowMap, opacity = widgets.IntSlider(min=0, max=100, step=1, value=70, continuous_update=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c3b6b2a272244b3b14d78256da537e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(basemap={'url': 'https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png', 'max_zoom': 19, 'attribution': 'Map …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mapOpacity = float(60)/100.\n",
    "map = Map(center=(47, -129), zoom=5, layout=Layout(width='100%', height='600px'))\n",
    "image = ImageOverlay(url=\"../../../../images/modis_sst.png\", bounds=((53.82, -138.58), (40.22, -122.45)), opacity=mapOpacity)\n",
    "map.add_layer(image);\n",
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.51, 11.27, 11.47, 10.66, 10.41, 10.41, 10.42, 10.48]\n"
     ]
    }
   ],
   "source": [
    "ca=kilroy.getCASites()\n",
    "\n",
    "# for i in ca: print(i[0], i[1], i[2])\n",
    "# for i in range(3): for j in range(3): print(m.MODISA_L3m_SST_2014_sst[0][i][j].values)\n",
    "# \n",
    "# testLat = ca[1][1]       #   44.51528\n",
    "# testLon = ca[1][2]       # -125.38972\n",
    "# print(testLat, testLon)\n",
    "\n",
    "sst = []\n",
    "for i in range(len(ca)):\n",
    "    # print(m.MODISA_L3m_SST_2014_sst.sel(lat=ca[i][1], lon=ca[i][2], method = 'nearest'))\n",
    "    temp = m.MODISA_L3m_SST_2014_sst.sel(lat=ca[i][1], lon=ca[i][2], method = 'nearest').values\n",
    "    sst.append(round(float(temp[0]), 2))\n",
    "\n",
    "# Here are MODIS-derived day-time sea surface temperatures at the cabled array on 2016-01-01T00:15:10 \n",
    "print(sst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read connection credentials from a non-repo location\n",
    "authfile=open('../../creds/ooi_creds','r')     # format of this file is username,token\n",
    "line=authfile.readline().rstrip()              # please note rstrip() removes any trailing \\n whitespace\n",
    "authfile.close()\n",
    "username,token = line.split(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to begin? Here: https://pypi.python.org/pypi/python-cmr/0.3.1\n",
    "# pip install is necessary here (not conda install) and this may or may not persist. \n",
    "# If the code below throws an error: Un-comment and run this line:\n",
    "# !pip install python-cmr\n",
    "\n",
    "# There are two types of queries: Collections (large scale) and Granules (targeted, spatially I think)\n",
    "from cmr import CollectionQuery, GranuleQuery\n",
    "capi = CollectionQuery() \n",
    "gapi = GranuleQuery()\n",
    "\n",
    "# You can run this and variants like capi.archive_center() to get a hint of how the CollectionQuery is put together\n",
    "# dir(capi)\n",
    "capi, gapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok so there are data centers called DAACs. How can I determine their names??' \n",
    "# Well this doesn't help much: \n",
    "#   dir(CollectionQuery().archive_center)\n",
    "# Instead search on EOSDIS and get the listing of DAACs. You will eventually find https://earthdata.nasa.gov/about/daacs.\n",
    "# The DAAC names given there are *mostly* accurate (terms in parens). \n",
    "# Put them in a list and we make a bit more progress\n",
    " \n",
    "collection_count = 0\n",
    " \n",
    "# This is a guess from the website; but it proves to be wrong in some particulars. \n",
    "# For example 'ORNL' returns 0 hits.\n",
    "# daacs = ['ASF', 'ASDC', 'CDDIS', 'GHRC', 'GES DISC', 'LP DAAC', 'LAADS', \\\n",
    "#          'NSIDC', 'ORNL', 'OB.DAAC', 'PO.DAAC', 'SEDAC']\n",
    "\n",
    "# This is an updated list of DAAC search strings\n",
    "# Now ORNL_DAAC returns 1287 forsooth\n",
    "daacs = ['ASF', 'ASDC', 'CDDIS', 'GHRC', 'GES_DISC', 'LP DAAC', 'LAADS', \\\n",
    "         'NSIDC', 'ORNL_DAAC', 'OB.DAAC', 'PO.DAAC', 'SEDAC']\n",
    "\n",
    "# the collections = line is a bit of magical thinking: asking for 20 thousand results... \n",
    "for daac in daacs: \n",
    "    collections = capi.archive_center(daac).keyword(\"*\").get(20000)\n",
    "    print (daac, 'returns', len(collections), 'collections')\n",
    "    collection_count += len(collections)\n",
    "\n",
    "print('\\nA total of', collection_count,'collections.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are they named, these collections? \n",
    "# Let's try ASF...\n",
    "collections = capi.archive_center('ASF').keyword(\"*\").get(20000)\n",
    "for c in collections: print(c['short_name'])\n",
    "print('\\n')\n",
    "# Let's try physical oceanography\n",
    "collections = capi.archive_center('PO.DAAC').keyword(\"*\").get(7)\n",
    "for c in collections: print(c['short_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's skip the DAAC name and just search on keywords\n",
    "collections = capi.keyword(\"*\").get(30000)\n",
    "# for c in collections: print(c['short_name'])\n",
    "# This returns 756 named collections:\n",
    "print(len(collections))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok let's get excited: Perhaps with the wildcard keyword we can assemble the proper DAAC names...\n",
    "daaclist = []\n",
    "for c in collections:\n",
    "    # This gets at originating orgs, not DAACs: if 'organizations' in c:\n",
    "    # if 'archive_center' in c: from 20000 asks we get 1132 responses from GSFC to HELSINKI... \n",
    "    if 'data_center' in c:\n",
    "        if c['data_center'] not in daaclist: \n",
    "            daaclist.append(c['data_center'])\n",
    "\n",
    "print(len(daaclist))\n",
    "print(daaclist)\n",
    "\n",
    "# This block of code gets rid of redundancies from above \n",
    "# daacs_redux = []\n",
    "# for org in daaclist:\n",
    "#     if org not in daacs_redux: daacs_redux.append(org)\n",
    "# \n",
    "# print(len(daacs_redux)) \n",
    "# The above line produces 2022 when we ask for 30000 collections (29998 have 'organizations' in the dict)\n",
    "#                         1139 when we ask for 20000 collections (1999)\n",
    "\n",
    "# this from the EOSDIS DAAC page is my initial guess of proper DAAC names\n",
    "# daacs = ['ASF', 'ASDC', 'CDDIS', 'GHRC', 'GES DISC', 'LAADS', 'LP DAAC', \\\n",
    "#                'NSIDC', 'ORNL', 'OB.DAAC', 'PO.DAAC', 'SEDAC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So now I think 'hey tides! that sounds good!'\n",
    "# I search on 'ALT_TIDE_GAUGE_L4_OST_SLA_US_WEST_COAST' and found out that podaac is at JPL. Go figure. \n",
    "#   https://podaac.jpl.nasa.gov/dataset/ALT_TIDE_GAUGE_L4_OST_SLA_US_WEST_COAST\n",
    "# How do I get some data to make a plot? It seems to be gridded.\n",
    "for c in collections: \n",
    "    if 'ALT_TIDE_GAUGE_L4_OST_SLA_US_WEST_COAST_DAILY' in c['short_name']:\n",
    "        #print(c['links'], c['organizations'][0])\n",
    "        print(c['organizations'][0])\n",
    "\n",
    "# From this I get a large blurp of JSON metadata including an ftp site: \n",
    "# ftp://podaac-ftp.jpl.nasa.gov/allData/coastal_alt/preview/L4/OSU_COAS/daily\n",
    "# So maybe the next thing I do is an 'ls -al' at that site to see what is available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking into a retrieved data file\n",
    "\n",
    "From the physical oceanography DAAC, mean sea level along the western UW coast in the directory /data/cmr.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok now it is time to get a data file\n",
    "# The process to arrive at the curl/wget commands below in this cell was to manually format the JSON into readable format.\n",
    "# That is the next cell below, raw text. This gives dictionary terms including ['links']. I did not automate this step however. \n",
    "# Rather I just looked at the URL and found the data files; and so copied the filename for the 1998 data.\n",
    "# Noticing: On the server they are about 7 MB and here on pangeo they inflate to 19MB but retain the .gz extension.\n",
    "# That is another time-wasting gotcha: They are *not* zipped and must simply be renamed fubar.nc\n",
    "# \n",
    "# !curl https://podaac-opendap.jpl.nasa.gov/opendap/allData/coastal_alt/preview/L4/OSU_COAS/daily/osu_cioss_daily_msla_geovel_1998_v1.nc.gz -o ./msla1998.nc\n",
    "# !curl https://podaac-opendap.jpl.nasa.gov/opendap/allData/coastal_alt/preview/L4/OSU_COAS/daily/osu_cioss_daily_msla_geovel_1999_v1.nc.gz -o ./msla1999.nc\n",
    "# !curl https://podaac-opendap.jpl.nasa.gov/opendap/allData/coastal_alt/preview/L4/OSU_COAS/daily/osu_cioss_daily_msla_geovel_2000_v1.nc.gz -o ./msla2000.nc\n",
    "# !curl https://podaac-opendap.jpl.nasa.gov/opendap/allData/coastal_alt/preview/L4/OSU_COAS/daily/osu_cioss_daily_msla_geovel_2001_v1.nc.gz -o ./msla2001.nc\n",
    "# !curl https://podaac-opendap.jpl.nasa.gov/opendap/allData/coastal_alt/preview/L4/OSU_COAS/daily/osu_cioss_daily_msla_geovel_2002_v1.nc.gz -o ./msla2002.nc\n",
    "# !curl https://podaac-opendap.jpl.nasa.gov/opendap/allData/coastal_alt/preview/L4/OSU_COAS/daily/osu_cioss_daily_msla_geovel_2003_v1.nc.gz -o ./msla2003.nc\n",
    "# !curl https://podaac-opendap.jpl.nasa.gov/opendap/allData/coastal_alt/preview/L4/OSU_COAS/daily/osu_cioss_daily_msla_geovel_2004_v1.nc.gz -o ./msla2004.nc\n",
    "# !curl https://podaac-opendap.jpl.nasa.gov/opendap/allData/coastal_alt/preview/L4/OSU_COAS/daily/osu_cioss_daily_msla_geovel_2005_v1.nc.gz -o ./msla2005.nc\n",
    "# !curl https://podaac-opendap.jpl.nasa.gov/opendap/allData/coastal_alt/preview/L4/OSU_COAS/daily/osu_cioss_daily_msla_geovel_2006_v1.nc.gz -o ./msla2006.nc\n",
    "# !curl https://podaac-opendap.jpl.nasa.gov/opendap/allData/coastal_alt/preview/L4/OSU_COAS/daily/osu_cioss_daily_msla_geovel_2007_v1.nc.gz -o ./msla2007.nc\n",
    "!curl https://podaac-opendap.jpl.nasa.gov/opendap/allData/coastal_alt/preview/L4/OSU_COAS/daily/osu_cioss_daily_msla_geovel_2008_v1.nc.gz -o ./msla2008.nc\n",
    "!curl https://podaac-opendap.jpl.nasa.gov/opendap/allData/coastal_alt/preview/L4/OSU_COAS/daily/osu_cioss_daily_msla_geovel_2009_v1.nc.gz -o ./msla2009.nc\n",
    "!curl https://podaac-opendap.jpl.nasa.gov/opendap/allData/coastal_alt/preview/L4/OSU_COAS/daily/osu_cioss_daily_msla_geovel_2010_v1.nc.gz -o ./msla2010.nc\n",
    "# !curl https://podaac-opendap.jpl.nasa.gov/opendap/allData/coastal_alt/preview/L4/OSU_COAS/daily/osu_cioss_daily_msla_geovel_2011_v1.nc.gz -o ./curla2011.nc\n",
    "# !curl https://podaac-opendap.jpl.nasa.gov/opendap/allData/coastal_alt/preview/L4/OSU_COAS/daily/osu_cioss_daily_msla_geovel_2012_v1.nc.gz -o ./curla2012.nc\n",
    "# !curl https://podaac-opendap.jpl.nasa.gov/opendap/allData/coastal_alt/preview/L4/OSU_COAS/daily/osu_cioss_daily_msla_geovel_2013_v1.nc.gz -o ./curla2013.nc\n",
    "# !curl https://podaac-opendap.jpl.nasa.gov/opendap/allData/coastal_alt/preview/L4/OSU_COAS/daily/osu_cioss_daily_msla_geovel_2014_v1.nc.gz -o ./curla2014.nc\n",
    "# !wget https://podaac-opendap.jpl.nasa.gov/opendap/allData/coastal_alt/preview/L4/OSU_COAS/daily/osu_cioss_daily_msla_geovel_1998_v1.nc.gz \n",
    "!ls -al"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Here is the JSON (reformatted to be readable) for a collection query result using python-cmr. \n",
    "It happens to come from the Physical Oceanography DAAC at JPL. And this is a raw nbconvert cell.\n",
    "You will see PO.DAAC and variants at multiple points in this metadata. \n",
    "Incidentally one of the links appears to point to an opendap server. \n",
    "\n",
    "{\n",
    " 'processing_level_id': '4', \n",
    " 'boxes': ['35 -133 49 -111'], \n",
    " 'time_start': '1992-10-14T00:00:00.000Z', \n",
    " 'version_id': '1', \n",
    " 'updated': '2011-01-21T17:10:13.000Z', \n",
    " 'dataset_id': 'PODAAC-USWCO-ALT01', \n",
    " 'has_spatial_subsetting': False, \n",
    " 'has_transforms': False, \n",
    " 'has_variables': False, \n",
    " 'data_center': 'PODAAC', \n",
    " 'short_name': 'ALT_TIDE_GAUGE_L4_OST_SLA_US_WEST_COAST', \n",
    " 'organizations': \n",
    "     [\n",
    "      'PO.DAAC', \n",
    "      'Physical Oceanography Distributed Active Archive Center, Jet Propulsion Laboratory, N', \n",
    "      'Craig Risin and P. Ted Strub'\n",
    "     ], \n",
    " 'title': 'PODAAC-USWCO-ALT01', \n",
    " 'coordinate_system': 'CARTESIAN', \n",
    " 'summary': 'This data set contains sea level anomalies and alongshore ocean currents (U and V) derived from satellite altimeters and tide gauge data.  \n",
    "             Currents and heights near the coast are calculated using tide', \n",
    " 'orbit_parameters': {}, \n",
    " 'id': 'C1292592421-PODAAC', \n",
    " 'has_formats': False, \n",
    " 'score': 2.8252225, \n",
    " 'original_format': 'ECHO10', \n",
    " 'archive_center': 'PO.DAAC', \n",
    " 'browse_flag': False, \n",
    " 'online_access_flag': True, \n",
    " 'links': \n",
    "     [\n",
    "         {\n",
    "             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#', \n",
    "             'hreflang': 'en-US', \n",
    "             'href': 'http://podaac-opendap.jpl.nasa.gov/opendap/allData/coastal_alt/preview/L4/OSU_COAS/weekly/'\n",
    "         }, \n",
    "         {\n",
    "             'rel': 'http://esipfed.org/ns/fedsearch/1.1/data#', \n",
    "             'hreflang': 'en-US', \n",
    "             'href': 'ftp://podaac-ftp.jpl.nasa.gov/allData/coastal_alt/preview/L4/OSU_COAS/weekly'\n",
    "         }, \n",
    "         {\n",
    "             'rel': 'http://esipfed.org/ns/fedsearch/1.1/documentation#', \n",
    "             'hreflang': 'en-US', \n",
    "             'href': 'http://podaac.jpl.nasa.gov/ws/search/granule/?datasetId=PODAAC-USWCO-ALT01&apidoc'\n",
    "         }\n",
    "     ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermezzo: Pushing data to the cloud\n",
    "\n",
    "Suppose from the above we have identified a Mean Surface Level Anomaly data file for 2010, seen locally\n",
    "as `./msla2010.nc`. We want to keep this available in an AWS S3 bucket with open (public) read access so \n",
    "that anyone can grab it, for example using boto. The next cell pushes this file into the cloud. However\n",
    "if will fail to run because I'm not putting the credentials file in place. The credentials give me permission\n",
    "to write to that S3 bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get our boto tools handy\n",
    "import boto\n",
    "import boto.s3\n",
    "import sys\n",
    "from boto.s3.key import Key\n",
    "\n",
    "# Read connection credentials from a file\n",
    "authfile=open('~/creds/s3auth.txt','r')\n",
    "line=authfile.readline().rstrip()    # please note rstrip() removes any trailing \\n whitespace\n",
    "authfile.close()\n",
    "AWS_ACCESS_KEY_ID,AWS_SECRET_ACCESS_KEY = line.split(',')\n",
    "\n",
    "# connect and establish a bucket object that we use for everything that follows\n",
    "connection = boto.connect_s3(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\n",
    "bucket_name = 'himatdata'\n",
    "bucket = connection.get_bucket(bucket_name)\n",
    "\n",
    "\n",
    "# point to the data file that we want to copy\n",
    "data_dir = '~/cahw2018_tutorials/rob/'\n",
    "datafile = data_dir + \"msla2010.nc\"\n",
    "print('Uploading %s to Amazon S3 bucket %s' % (datafile, bucket_name))\n",
    "\n",
    "# This is a cute little .... progress bar thingy\n",
    "def percent_cb(complete, total):\n",
    "    sys.stdout.write('.')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# The Key is a pointer to an S3 object. \n",
    "#   It has a '.key' which is the fullpath/filename of that object.\n",
    "#   It is capable of executing the bytes-transfer file copy. (The bucket connection object does not do this.)\n",
    "k = Key(bucket)\n",
    "k.key = 'cmr/msla2010.nc'   # notice I am adding a subdirectory to the object path, 'cmr'\n",
    "k.set_contents_from_filename(datafile, cb=percent_cb, num_cb=10)\n",
    "\n",
    "# Let's look for our file in the S3 bucket now that it is copied over\n",
    "print('\\n')\n",
    "for key in bucket.list():\n",
    "    object_name = str(key.name.encode('utf-8'))\n",
    "    if 'msla' in object_name: print(object_name)\n",
    "\n",
    "# Suppose we also accidentally made a copy of our object in the root of s3 and we want to delete it...\n",
    "#   This code will delete it and re-list all *msla* objects in s3 to verify it is gone; but the one \n",
    "#   we want is still there. \n",
    "# k.key = 'msla1998.nc'\n",
    "# bucket.delete_key(k)\n",
    "# print('\\n')\n",
    "# for key in bucket.list():\n",
    "#     object_name = str(key.name.encode('utf-8'))\n",
    "#     if 'msla' in object_name: print(object_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining our MSLA data file\n",
    "\n",
    "We used CMR to identify and pull a .nc file of mean sea level anomaly off the western coast of the US. \n",
    "This 20MB file is mean sea level anomaly file is gridded and also contains geostrophic currents.\n",
    "The data are for 1998 on a daily average basis. How do we know this? Well we start using Python \n",
    "tools to peer under the hood so to speak. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import netCDF4\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will attach an xarray object called 'm' to the data file; this is the key action\n",
    "data_dir = './'\n",
    "msla_file = data_dir + 'msla2010.nc'\n",
    "\n",
    "# m = xr.open_mfdataset(msla_file, decode_cf=False)\n",
    "m = xr.open_mfdataset(msla_file)\n",
    "\n",
    "# 'm' is an xarray dataset. By placing m on a line by itself we trigger its 'spill my guts' method, behold...\n",
    "m   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at a couple data values\n",
    "print('anomaly height')\n",
    "print(m['sea_surface_height_above_sea_level'][0:2,4:6,1:3].values) \n",
    "print('\\n')\n",
    "print('velocity-north')\n",
    "print(m['surface_geostrophic_northward_sea_water_velocity'][0:2,4:6,1:3].values) \n",
    "print('\\n')\n",
    "print('velocity-east')\n",
    "print(m['surface_geostrophic_eastward_sea_water_velocity'][0:2,4:6,1:3].values) \n",
    "print('\\n')\n",
    "# and while we are here -- on our way to a 2D plot (lat/lon) for a given time using .sel(time):\n",
    "# Let's get it to cough up some time values\n",
    "print('datetimes')\n",
    "print (m['time'].values[10:15])\n",
    "# print ('    ')\n",
    "# print (m['time'].values)\n",
    "\n",
    "# This is a terrible, kludgy idea; don't do it\n",
    "# custom_dts=m['time'].values[0:364]\n",
    "# print(custom_dts[219])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(m.time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These imports give us control sliders that we use for selecting depth slices from the dataset\n",
    "#   (a more ambitious soul might go for a JSAnim movie)\n",
    "from ipywidgets import *\n",
    "from traitlets import dlink\n",
    "\n",
    "import datetime as dt\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_object_list = []\n",
    "\n",
    "def plotMSLA(date_value): \n",
    "    # does not use year, month, day\n",
    "    # the_date = dt.datetime(year, month, day, 12)\n",
    "    # print (the_date)\n",
    "    msla=m['sea_surface_height_above_sea_level'].sel(time=date_value)\n",
    "    # msla=m['sea_surface_height_above_sea_level'].sel(time='1998-01-15T12:00:54.996336640')\n",
    "    \n",
    "    # set us up to record this figure in the list of figures\n",
    "    thisFigure, ax = plt.subplots(1,figsize=(16, 10))\n",
    "\n",
    "    # The following works by using a dt from the Dataset (see above)\n",
    "    # msla=m['sea_surface_height_above_sea_level'].sel(time='1998-01-15T12:00:54.996336640')\n",
    "    # msla=m['sea_surface_height_above_sea_level'].sel(time=10237.5)\n",
    "    # msla will now be a DataArray derived from the Dataset; you can... print(msla)\n",
    "    msla.plot(ax=ax, x='longitude', y='latitude', cmap=plt.cm.bwr, vmin=-.15,vmax=.15)\n",
    "    # it would be pleasant to rescale the longitude to [227.75,240.] (and to use west longitude for that matter)\n",
    "    # Some fossil code for labeling the chart (makes sense in concert with a chooser widget)\n",
    "    msg1 = '   Mean sea-level anomaly' \n",
    "    msg2 = '(daily average) on ' + str(date_value).split('T')[0]\n",
    "    plt.text(238, 46, msg1, fontsize = '20')\n",
    "    plt.text(238, 45, msg2, fontsize = '20')\n",
    "    figure_object_list.append(thisFigure)\n",
    "    plt.close(thisFigure)\n",
    "    \n",
    "# def plot_that_msla(date_index):\n",
    "#     plotMSLA(date_index)\n",
    "\n",
    "# This is the interactive slider\n",
    "# interact(plot_that_msla, date_index=widgets.IntSlider(min=0,max=364,step=1,value=0, continuous_update=False))\n",
    "\n",
    "for v in m.time.values:\n",
    "    plotMSLA(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(figure_object_list))\n",
    "print(figure_object_list[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import animation_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_image(fig, **kwargs):\n",
    "    \"\"\"\n",
    "    Take a matplotlib figure *fig* and convert it to an image *im* that \n",
    "    can be viewed with imshow.\n",
    "    \"\"\"\n",
    "\n",
    "    import io\n",
    "    png = io.BytesIO()\n",
    "    fig.savefig(png,format='png', **kwargs)\n",
    "    png.seek(0)\n",
    "    im = plt.imread(png)\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_images(figs, **kwargs):\n",
    "    \"\"\"\n",
    "    Take a list of matplotlib figures *figs* and convert to list of images.\n",
    "    \"\"\"\n",
    "\n",
    "    images = []\n",
    "    for fig in figs:\n",
    "        im = make_image(fig, **kwargs)\n",
    "        images.append(im)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These imports give us control sliders that we use for selecting depth slices from the dataset\n",
    "#   (a more ambitious soul might go for a JSAnim movie)\n",
    "# Might be necessary to install JSAnimation:\n",
    "#   conda install -y -c conda-forge jsanimation\n",
    "# import animation_tools\n",
    "images = make_images(figure_object_list, dpi=150)\n",
    "# animation_tools.JSAnimate_images(images, figsize=(16,12))\n",
    "\n",
    "from matplotlib import animation\n",
    "\n",
    "fig = plt.figure(figsize=(16,12), dpi=None)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')  # so there's not a second set of axes\n",
    "\n",
    "im = plt.imshow(images[0])\n",
    "\n",
    "def init():\n",
    "    im.set_data(images[0])\n",
    "    return im,\n",
    "\n",
    "def animate(i):\n",
    "    im.set_data(images[i])\n",
    "    return im,\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init, frames=len(images), interval=200, blit=True)\n",
    "\n",
    "plt.show()\n",
    "anim.save('mymovie.mp4',fps=15)\n",
    "\n",
    "# def init():\n",
    "#     im.set_data(figure_object_list[0])\n",
    "#     return im,\n",
    "\n",
    "# def animate(i):\n",
    "#     im.set_data(figure_object_list[i])\n",
    "#     return im,\n",
    "\n",
    "# from matplotlib import animation \n",
    "# # anim = animation.FuncAnimation(figure_object_list, \\\n",
    "# #                                animate, \\\n",
    "# #                                init_func=init, \\\n",
    "#                                frames=len(figure_object_list), \\\n",
    "#                                interval=200, \\\n",
    "#                                blit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector field render\n",
    "\n",
    "Since we are very interested in water sloshing around let's take a moment to learn about plotting a vector field.\n",
    "Our msla1998.nc file has both northward and eastward surface water velocities so we're set for data. \n",
    "(BTW 'geostrophic' means that pressure gradients and coriolus forces are in play.)\n",
    "\n",
    "\n",
    "First I'll insert a block of code taken from [this page](https://matplotlib.org/examples/pylab_examples/quiver_demo.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import ma\n",
    "X, Y = np.meshgrid(np.arange(0, 2 * np.pi, .2), np.arange(0, 2 * np.pi, .2))\n",
    "U, V = np.cos(X), np.sin(Y)\n",
    "plt.figure(figsize=(10,6))\n",
    "Q = plt.quiver(X, Y, U, V, units='width')\n",
    "qk = plt.quiverkey(Q, 0.9, 0.9, 2, r'$2 \\frac{m}{s}$', labelpos='E', coordinates='figure')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let us try and adopt this code to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import ma\n",
    "\n",
    "def ShowCurrents(date_index):\n",
    "    custom_dts=m['time'].values[:]\n",
    "    # X, Y = np.meshgrid(m['longitude'], m['latitude'])\n",
    "    X, Y = np.meshgrid(m['latitude'], m['longitude'])\n",
    "    U = m['surface_geostrophic_northward_sea_water_velocity'].sel(time=custom_dts[date_index])\n",
    "    V = m['surface_geostrophic_eastward_sea_water_velocity'].sel(time=custom_dts[date_index])\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title('Geostrophic surface currents')\n",
    "    thisFigure = plt.quiver(Y, X, V, U, units='width')\n",
    "    # qk = plt.quiverkey(Q, 0.9, 0.9, 2, r'$2 \\frac{m}{s}$', labelpos='E', coordinates='figure')\n",
    "    # msg1 = '   Geostrophic surface currents' \n",
    "    # msg2 = '(daily average) on ' + str(m['time'][date_index]).split('T')[0]\n",
    "    # plt.text(238, 46, msg1, fontsize = '20')\n",
    "    # plt.text(238, 45, msg2, fontsize = '20')\n",
    "    # plt.show()\n",
    "    \n",
    "# This is the interactive slider\n",
    "interact(ShowCurrents, date_index=widgets.IntSlider(min=0,max=364,step=1,value=0, continuous_update=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This used to work...\n",
    "# U.plot(x='longitude',y='latitude')\n",
    "plt.imshow(U,x='longitude',y='latitude')\n",
    "# Uxr = xr.DataArray(U)\n",
    "# Uxr.plot(x='longitude', y='latitude')\n",
    "# Uxr.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V.plot(x='longitude', y='latitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x=m.coords['longitude'].values\n",
    "y=m.coords['latitude'].values\n",
    "\n",
    "print(len(x))\n",
    "print(len(y))\n",
    "print 54*84\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import ma\n",
    "import datetime as dt\n",
    "import pandas\n",
    "\n",
    "# geostrophic surface currents\n",
    "GSC_figobjlist = []\n",
    "\n",
    "def plotGSC(date_value): \n",
    "\n",
    "    # from ShowCurrents(date_index)\n",
    "    custom_dts=m['time'].values[:]\n",
    "    X, Y = np.meshgrid(m['latitude'], m['longitude'])\n",
    "    U = m['surface_geostrophic_northward_sea_water_velocity'].sel(time=date_value)\n",
    "    V = m['surface_geostrophic_eastward_sea_water_velocity'].sel(time=date_value)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title('Geostrophic surface currents')\n",
    "    thisFigure = plt.quiver(Y, X, V, U, units='width')\n",
    "    # I did not follow the msla recipe of thisFigure, ax = plt.subplots(etc)\n",
    "    # msg1 = '   Geostrophic surface currents' \n",
    "    # msg2 = '(daily average) on ' + str(date_value).split('T')[0]\n",
    "    # plt.text(238, 46, msg1, fontsize = '20')\n",
    "    # plt.text(238, 45, msg2, fontsize = '20')\n",
    "    GSC_figobjlist.append(thisFigure)\n",
    "    # plt.close(thisFigure)\n",
    "    \n",
    "    # original\n",
    "    # msla=m['sea_surface_height_above_sea_level'].sel(time=date_value)\n",
    "    # thisFigure, ax = plt.subplots(1,figsize=(16, 10))\n",
    "    # msla.plot(ax=ax, x='longitude', y='latitude', cmap=plt.cm.bwr, vmin=-.15,vmax=.15)\n",
    "   \n",
    "# for v in m.time.values: \n",
    "    # plotGSC(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import animation_tools\n",
    "images = animation_tools.make_images(GSC_figobjlist, dpi=150)\n",
    "# animation_tools.JSAnimate_images(images, figsize=(16,12))\n",
    "\n",
    "from matplotlib import animation\n",
    "\n",
    "fig = plt.figure(figsize=(16,12), dpi=None)\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "ax.axis('off')  # so there's not a second set of axes\n",
    "\n",
    "im = plt.imshow(images[0])\n",
    "\n",
    "def init():\n",
    "    im.set_data(images[0])\n",
    "    return im,\n",
    "\n",
    "def animate(i):\n",
    "    im.set_data(images[i])\n",
    "    return im,\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init, frames=len(images), interval=200, blit=True)\n",
    "\n",
    "plt.show()\n",
    "anim.save('GSCmovie.mp4',fps=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Working with MODIS from Goddard's CES DISC \n",
    "\n",
    "g4.subsetted.MODISA_L3m_SST_2014_sst.20160101.138W_40N_122W_54N.nc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This section works with the granule api\n",
    "\n",
    "It does not do much at the moment except hang...\n",
    "\n",
    "kilroy go back to the python-cmr GitHub and look at the example; refine as needed or complain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "granules = gapi.short_name(\"AST_L1T\").point(-112.73, 42.5).get(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for granule in granules: print(granule[\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for granules matching a specific product/short_name\n",
    "gapi.short_name(\"AST_L1T\")\n",
    "\n",
    "# search for granules matching a specific version\n",
    "gapi.version(\"006\")\n",
    "\n",
    "# search for granules at a specific longitude and latitude\n",
    "gapi.point(-112.73, 42.5)\n",
    "\n",
    "# search for granules in an area bound by a box (lower left lon/lat, upper right lon/lat)\n",
    "gapi.bounding_box(-112.70, 42.5, -110, 44.5)\n",
    "\n",
    "# search for granules in a polygon\n",
    "gapi.polygon([(-100, 40), (-90, 40), (-95, 38), (-100, 40)])\n",
    "\n",
    "# search for granules in a line\n",
    "gapi.line([(-100, 40), (-90, 40), (-95, 38)])\n",
    "\n",
    "import datetime\n",
    "\n",
    "# search for granules in an open or closed date range\n",
    "gapi.temporal(\"2016-10-10T01:02:00Z\", \"2016-10-12T00:00:30Z\")\n",
    "gapi.temporal(\"2016-10-10T01:02:00Z\", None)\n",
    "gapi.temporal(datetime.datetime(2016, 10, 10, 1, 2, 0), datetime.datetime.now())\n",
    "# api.temporal([datetime.datetime(\"2016-10-10T01:02:00Z\"), datetime.datetime.now()])\n",
    "# api.temporal([datetime.datetime(2016, 10, 10), datetime.datetime.now()])\n",
    "\n",
    "# only include granules available for download\n",
    "# api.downloadable()\n",
    "\n",
    "# only include granules that are unavailable for download\n",
    "# api.online_only()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for a granule by its unique ID\n",
    "gapi.granule_ur(\"SC:AST_L1T.003:2150315169\")\n",
    "\n",
    "# search for granules from a specific orbit\n",
    "gapi.orbit_number(5000)\n",
    "\n",
    "# filter by the day/night flag\n",
    "gapi.day_night_flag(\"day\")\n",
    "\n",
    "# filter by cloud cover percentage range\n",
    "gapi.cloud_cover(25, 75)\n",
    "\n",
    "# filter by specific instrument or platform\n",
    "gapi.instrument(\"MODIS\")\n",
    "gapi.platform(\"Terra\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
